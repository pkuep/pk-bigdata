
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Query and Extraction: Extracting Data with sqoop&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="query_and_extraction_sqoop_integration_rdbms"
                  title="Lab &#34;Query and Extraction: Extracting Data with sqoop&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>In this use case we want to extract data from our data lake:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\74b3d1ebf9ffa90.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Writing data from the data lake back into operational systems, e.g. an RDBMS.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Set up a DataProc cluster which is capable of executing the sqoop job (sqoop is used for the next lab). SEE PREREQUISITES!</li>
<li>Writing the results of our batch processing back into the source system (RDBMS).</li>
</ul>
<aside class="warning"><p><strong>Please note:</strong> You&#39;ll need GCP access for this lab. The provided voucher needs to be redeemed. Remember to shut down the running cloud function after completing!</p>
</aside>
<aside class="warning"><p><strong>Prerequisites: </strong>The lab &#34;Big Data Course - Codelab - Query and Extraction - Data Lake Queries from Spark SQL&#34; needs to be executed first. There, we set up a cluster as follows and performed some simple batch computations.</p>
</aside>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a Hadoop cluster (DataProc) which provides functionality for accessing cloud SQL (initialization action &#34;cloud-sql-proxy&#34;) as well as the Jupyter notebook components being necessary for PySpark..</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Integration of Downstream Systems" duration="5">
        <h2 is-upgraded><strong>Creating the sales_statistics_region table in the RDBMS</strong></h2>
<p>First, we need to create the table in DBeaver for your user. Please create it as follows such that we can let sqoop push data into it:</p>
<pre><code>CREATE TABLE sales_statistics_region (
region VARCHAR(20),
average_sales FLOAT
);</code></pre>
<h2 is-upgraded><strong>Opening the master node&#39;s shell</strong></h2>
<p>Please navigate to the cluster&#39;s master node and open an SSH connection:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\a64e86bda4c2944b.png"></p>
<p>This will again be our &#34;entry point&#34; to the sqoop tool (remember: we added it via the initialization action). With the following command, we can write the results back to the SQL server:</p>
<pre><code>sqoop export jdbc:mysql://&lt;db-IP see Moodle&gt;/&lt;dbname, see Moodle&gt; --userna
me &lt;see Moodle&gt; --password &lt;see Moodle&gt; --table sales_statistics_region --hcatalog-table sales_statistics_region</code></pre>
<p>The parameter &#34;hcatalog-table&#34; determines which table from the data lake should be read. The parameter &#34;table&#34; directs towards the RDBMS&#39;s target table.</p>
<aside class="warning"><p><strong>Important: </strong>Due to configuration issues of DataProc (especially regarding Parquet), this command will not work. We will not go further into details on this here.</p>
</aside>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You learned how to extract data from the data lake via sqoop and push it back into RDBMS.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Cleaning Up" duration="1">
        <p>Please do not delete your cluster and SQL instances!</p>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="2">
        <p>You now know how extract data from the data lake and integrate &#34;traditional&#34; RDBMS into our big data architecture.</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
