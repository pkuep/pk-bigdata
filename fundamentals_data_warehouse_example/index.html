
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Fundamentals: Data Warehouse Example&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="fundamentals_data_warehouse_example"
                  title="Lab &#34;Fundamentals: Data Warehouse Example&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>This is again our use case: a simple webshop for which we now want to generate a very simple BI architecture, i.e. a data warehouse in the &#34;traditional&#34; manner.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\3c3e14c0a31b3057.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Loading example source data from cloud storage to a DWH Staging area, transforming the data to a star schema and providing the results in a DWH-like manner.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Load sample data from cloud storage to BigQuery (could be automated in a batch run)</li>
<li>Transform the data into a very simple star schema.</li>
<li>Reading the DWH tables from Jupyter Notebook.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Data Upload to Staging Area" duration="2">
        <p>Please navigate to &#34;Cloud Storage&#34; in the console and create a bucket (name it &#34;hdm-yourlastname&#34;, change to your last name):</p>
<p class="image-container"><img style="width: 337.50px" src="img\\1649b2c0c8f1e695.png"></p>
<p>Click on &#34;Create&#34;:</p>
<p class="image-container"><img style="width: 383.74px" src="img\\ed9ab83ecbb09dc3.png"></p>
<p>Upload the file webshop_history.csv (see Moodle) to your bucket:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\9ec596108a852515.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="BigQuery Dataset Creation and Staging Area" duration="4">
        <h2 is-upgraded><strong>Data upload and dataset creation</strong></h2>
<p>Please go to BigQuery in the console:</p>
<p class="image-container"><img style="width: 290.00px" src="img\\eea0ac8b27dca3f5.png"></p>
<p>If not already selected, please select our project in the &#34;Explorer&#34;:</p>
<p class="image-container"><img style="width: 617.00px" src="img\\79c1d8679eb887ba.png"></p>
<p>We now want to &#34;Add Data&#34;:</p>
<p class="image-container"><img style="width: 605.00px" src="img\\99e572efcbe54c7d.png"></p>
<p>Call it wh_yourlastname. Next, select &#34;Create table&#34; using the menu next to your dataset&#39;s name:</p>
<p class="image-container"><img style="width: 404.00px" src="img\\ff858057cd36bdea.png"></p>
<p>Select &#34;Google Cloud Storage&#34; as data source and set the path to your webshop_history.csv in your bucket:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\51c196b0226154b5.png"></p>
<p>You can use &#34;Browse&#34; to be able to navigate to your bucket:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\b4fcd23a02082ff2.png"></p>
<p>Navigate to the webshop_datalake and select the csv-file:</p>
<p class="image-container"><img style="width: 399.00px" src="img\\40cb15be30d7c5ed.png"></p>
<p>Please do not forget to set Schema auto detection.</p>
<h2 is-upgraded><strong>First data inspection</strong></h2>
<p>You should be able to click on the table sales in the navigation menu:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\d56d79061232c5f1.png"></p>
<p>You can execute a sample SQL query by clicking on the Query button:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\b713428c00b2e184.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a dataset for our sample data warehouse.</li>
<li>You created a table from a csv file in the data lake. This step could be automated and new data, for example, be appended to the table.</li>
<li>You took a look at the data via the &#34;preview&#34; function and inspected the table&#39;s schema for correctness.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Data Transformation (Star Schema - Product Dimension)" duration="8">
        <aside class="warning"><p><strong>Please note: </strong>Due to simplification of the labs, our data are stored in the csv-file in a de-normalized manner, i.e. just a flat file that contains duplicates (e.g. the product_name is repeated several times). From a modeling point of view, we usually deal with normalized data structures in source systems, especially in OLTP-systems (like a sales system). Normalized data (=the relational model) avoid redundancies and inconsistencies, e.g. by putting the product master data into its own table. Actually, BigQuery performs better with de-normalized data and we could just leave the data as they are. However, for demonstration purposes we create a star schema based on our staging table.</p>
</aside>
<h2 is-upgraded><strong>Querying the staging table for products</strong></h2>
<p>Use this SQL code to derive the dimension table &#34;product&#34; and run the query.</p>
<pre><code>-- derive the dimension table product from the source data
SELECT DISTINCT product, product_name FROM sales</code></pre>
<p>You should see the results in the lower part of the window:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\392d63b208064009.png"></p>
<h2 is-upgraded><strong>Saving the results as a dimension table &#34;product&#34;</strong></h2>
<p>Select &#34;Save results&#34; and save the query result as a new BigQuery table calles &#34;dwh_product&#34;:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\64223b0aca31e078.png"></p>
<p>Store this new table in the same dataset:</p>
<p class="image-container"><img style="width: 577.00px" src="img\\37f207c971a8471e.png"></p>
<aside class="warning"><p><strong>Please note: </strong>We could also save this product dimension table as a &#34;view&#34;. For demonstration purposes, we however want to &#34;move the data physically&#34; to a new DWH-table (just as we did it by saving the results).</p>
</aside>
<h2 is-upgraded><strong>Scheduling the query</strong></h2>
<p>In order to set up a productive architecture, we&#39;d need to schedule the query to be executed, e.g. every night after uploading new data to the sales_staging table (which we&#39;ll not implement).</p>
<p>Select &#34;Schedule&#34;:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\2831eb197ad70fc3.png"></p>
<p class="image-container"><img style="width: 566.00px" src="img\\28cecda245400080.png"></p>
<p>We can set the repeats (e.g. daily), set the destination table (e.g. our &#34;dim_product&#34;) and define, if we want to append or overwrite data (in our case overwrite would be necessary for the query).</p>
<p>We won&#39;t set up the schedule now.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a query to derive a dimension table from a flat (de-normalized) table.</li>
<li>You saved this table and thus built the first part of our exemplary simplified star schema.</li>
<li>You learned how to schedule queries for operative use.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Your Task - Data Transformation (Star Schema - Sales Facts)" duration="4">
        <h2 is-upgraded><strong>Querying the staging table for sales transactions (facts)</strong></h2>
<p>Similar to the dimension table generation, your task is now to derive the sales facts from the staging table. Generate a new table dwh_sales which contains the date, product, and sales_value columns by querying the staging table.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>After having completed this task you added another part to our simple star schema.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Solution - Data Transformation (Star Schema - Sales Facts)" duration="4">
        <h2 is-upgraded><strong>Query</strong></h2>
<p>Your query should look like this:</p>
<pre><code>-- derive the sales facts from the source data
SELECT date, product, sales_value FROM `hdm-bi-bigdata.wh_kueppers.sales`;</code></pre>
<p>After creating the fact_sales as a table &#34;wh_kueppers&#34; you should see three tables:</p>
<p class="image-container"><img style="width: 284.00px" src="img\\e8a54f5c982ab84d.png"></p>
<aside class="warning"><p><strong>Please note: </strong>BigQuery&#39;s pricing model is based on the amount of data your queries &#34;touch&#34;. Thus, you can estimate how expensive your ETL (extract, transform, load) architecture will be, e.g. using the GCP pricing calculator (<a href="https://cloud.google.com/products/calculator" target="_blank">https://cloud.google.com/products/calculator</a>). Under &#34;Execution Details&#34;, you will find some information on the &#34;Bytes shuffled&#34;.</p>
</aside>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You now completed the simplified star schema.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Querying the DWH from the Jupyter Notebook" duration="2">
        <h2 is-upgraded><strong>BigQuery connection</strong></h2>
<p>Please open your sample notebook from the last lab. Since the notebook is running in the same project as our bigquery dataset, we can immediately connect to the dataset:</p>
<pre><code>import pandas_gbq

# Parameter
PROJECT_ID = &#39;hdm-bi-bigdata&#39;
QUERY = &#34;&#34;&#34;
    SELECT *
    FROM `hdm-bi-bigdata.wh_kueppers.sales`
    LIMIT 10
&#34;&#34;&#34;

df = pandas_gbq.read_gbq(QUERY, project_id=PROJECT_ID)

df</code></pre>
<h2 is-upgraded><strong>Notes on BigQuery</strong></h2>
<p>You will probably not notice any costs for BigQuery throughout this course, since you&#39;ll stay below the threshold of queries that are for free. Also, we are storing such low data volumes in our use cases currently, that the storage costs will be negligible.</p>
<p>Furthermore you might have noticed that we are not connecting BigQuery to our MySQL database. BigQuery is a serverless data warehouse â€” it doesn&#39;t actively pull data from other systems but instead loads data from sources like Google Cloud Storage (GCS), Cloud SQL (via federated queries), other BigQuery datasets, or external sources through External Tables, BigLake Tables, or Connections. There is no native direct connection for MySQL running on a VM. To use that data in BigQuery, you need to build an intermediate step like exporting to GCS or using a data pipeline.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="1">
        <p>Congratulations, you completed the third lab in our big data fundamentals course. This lab focused on setting up components for a more &#34;traditional&#34; BI architecture. This is, however, very important since the big data architecture will augment the BI architecture, but not replace it completely. Knowing how to set up &#34;normal&#34; ETL processes is very important for all analytics-related projects.</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab. You may keep all components since they do not induce huge costs or costs at all.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
