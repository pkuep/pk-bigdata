
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Fundamentals: Data Warehouse Example&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="fundamentals_data_warehouse_example"
                  title="Lab &#34;Fundamentals: Data Warehouse Example&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>This is again our use case: a simple webshop for which we now want to generate a very simple BI architecture, i.e. a data warehouse in the &#34;traditional&#34; manner.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\3c3e14c0a31b3057.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Loading example source data from cloud storage to a DWH Staging area, transforming the data to a star schema and providing the results in a DWH-like manner.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Load sample data from cloud storage to BigQuery (could be automated in a batch run)</li>
<li>Transform the data into a very simple star schema.</li>
<li>Reading the DWH tables from Data Studio.</li>
</ul>
<aside class="warning"><p><strong>Please note:</strong> You&#39;ll need GCP access for this lab. The provided voucher needs to be redeemed.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Dataset Creation and Data Upload to Staging Area" duration="4">
        <h2 is-upgraded><strong>Creation of a dataset</strong></h2>
<p>Please go to BigQuery in the console:</p>
<p class="image-container"><img style="width: 249.50px" src="img\\4f0d07bd7d301f01.png"></p>
<p>Select your project in the &#34;Resources&#34; list:</p>
<p class="image-container"><img style="width: 224.61px" src="img\\7fd58d8cf352d5a9.png"></p>
<p>Create a new dataset &#34;example_dwh&#34;:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\f5d85abd233f2e07.png"></p>
<p>Leave all other settings as default and hit &#34;Create dataset&#34;. </p>
<p class="image-container"><img style="width: 229.67px" src="img\\66e5fab4569d8f8.png"></p>
<h2 is-upgraded><strong>Data upload</strong></h2>
<p>Select the newly created dataset</p>
<p class="image-container"><img style="width: 281.50px" src="img\\31f6207377a04d41.png"></p>
<p>Click on &#34;Create Table&#34;:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\1940fd235a3b431a.png"></p>
<p>Select &#34;Create table from:&#34; â†’ &#34;Google Cloud Storage&#34;. Here we uploaded our webshop data previously.</p>
<p class="image-container"><img style="width: 260.83px" src="img\\f0ec1d70d2870e2b.png"></p>
<p>Hit &#34;Browse&#34; and by double clicking navigate to the webshop_datalake (see previous lab).</p>
<p class="image-container"><img style="width: 258.50px" src="img\\b62cc11330635f33.png"></p>
<p>Select &#34;webshop_history.csv&#34;.</p>
<p>Call the new table &#34;sales_staging&#34; and select &#34;Auto detect Schema and input parameters&#34;:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\7bc4674f19212442.png"></p>
<p>All other settings can stay as default.</p>
<p>Hit &#34;Create table&#34;. The data from the csv file should now be available. Click on the table name &#34;sales_staging&#34;:</p>
<p class="image-container"><img style="width: 324.50px" src="img\\d84043bebea5f184.png"></p>
<p>You can take a look at the schema (check if the data types are as expected), preview data, or query the table </p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a dataset for our sample data warehouse.</li>
<li>You created a table from a csv file in the data lake. This step could be automated and new data, for example, be appended to the table.</li>
<li>You took a look at the data via the &#34;preview&#34; function and inspected the table&#39;s schema for correctness.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Data Transformation (Star Schema - Product Dimension)" duration="8">
        <aside class="warning"><p><strong>Please note: </strong>Due to simplification of the labs, our data are stored in the csv-file in a de-normalized manner, i.e. just a flat file that contains duplicates (e.g. the product_name is repeated several times). From a modeling point of view, we usually deal with normalized data structures in source systems, especially in OLTP-systems (like a sales system). Normalized data (=the relational model) avoid redundancies and inconsistencies, e.g. by putting the product master data into its own table. Actually, BigQuery performs better with de-normalized data and we could just leave the data as they are. However, for demonstration purposes we create a star schema based on our staging table.</p>
</aside>
<h2 is-upgraded><strong>Querying the staging table for products</strong></h2>
<p>Select the table &#34;sales_staging&#34; and hit &#34;Query table&#34;. A sample query is created. </p>
<p class="image-container"><img style="width: 624.00px" src="img\\3b100604b430889d.png"></p>
<p>Use this SQL code to derive the dimension table &#34;product&#34; and run the query.</p>
<pre><code>-- derive the dimension table product from the source data
SELECT DISTINCT product, product_name FROM `pk-bigdata.example_dwh.sales_staging`</code></pre>
<p>You should see the results in the lower part of the window:</p>
<p class="image-container"><img style="width: 469.59px" src="img\\15482363c04b13d5.png"></p>
<h2 is-upgraded><strong>Saving the results as a dimension table &#34;product&#34;</strong></h2>
<p>Select &#34;Save results&#34; and save the query result as a new BigQuery table calles &#34;dwh_product&#34;:</p>
<p class="image-container"><img style="width: 367.64px" src="img\\e9adc9de7d68ba07.png"></p>
<aside class="warning"><p><strong>Please note: </strong>We could also save this product dimension table as a &#34;view&#34;. For demonstration purposes, we however want to &#34;move the data physically&#34; to a new DWH-table (just as we did it by saving the results).</p>
</aside>
<h2 is-upgraded><strong>Scheduling the query</strong></h2>
<p>In order to set up a productive architecture, we&#39;d need to schedule the query to be executed, e.g. every night after uploading new data to the sales_staging table (which we&#39;ll not implement).</p>
<p>First, hit &#34;Schedule query&#34; and enable the API:</p>
<p class="image-container"><img style="width: 484.50px" src="img\\c366aab9b3238778.png"></p>
<p>Afterwards, take a look at the options you get when &#34;creating a new scheduled query&#34;:</p>
<p class="image-container"><img style="width: 425.50px" src="img\\702ec515d49470dc.png"></p>
<p>We can set the repeats (e.g. daily), set the destination table (e.g. our &#34;dwh_product&#34;) and define, if we want to append or overwrite data (in our case overwrite would be necessary for the query).</p>
<p>We won&#39;t set up the schedule now.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a query to derive a dimension table from a flat (de-normalized) table.</li>
<li>You saved this table and thus built the first part of our exemplary simplified star schema.</li>
<li>You learned how to schedule queries for operative use.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Your Task - Data Transformation (Star Schema - Sales Facts)" duration="4">
        <h2 is-upgraded><strong>Querying the staging table for sales transactions (facts)</strong></h2>
<p>Similar to the dimension table generation, your task is now to derive the sales facts from the staging table. Generate a new table dwh_sales which contains the date, product, and sales_value columns by querying the staging table.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>After having completed this task you added another part to our simple star schema.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Solution - Data Transformation (Star Schema - Sales Facts)" duration="4">
        <h2 is-upgraded><strong>Query</strong></h2>
<p>Your query should look like this:</p>
<pre><code>-- derive the sales facts from the source data
SELECT date, product, sales_value FROM `pk-bigdata.example_dwh.sales_staging`;</code></pre>
<p>Your dataset &#34;example_dwh&#34; now should contain three tables.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\8933ead22af14bcf.png"></p>
<aside class="warning"><p><strong>Please note: </strong>BigQuery shows on the right side for each query how much data will be processed. BigQuery&#39;s pricing model is based on the amount of data your queries &#34;touch&#34;. Thus, you can estimate how expensive your ETL (extract, transform, load) architecture will be, e.g. using the GCP pricing calculator (<a href="https://cloud.google.com/products/calculator" target="_blank">https://cloud.google.com/products/calculator</a>).</p>
</aside>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You now completed the simplified star schema.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Querying the DWH from Data Studio" duration="2">
        <h2 is-upgraded><strong>Dashboard Generation and BigQuery connection</strong></h2>
<p>Go to <a href="https://datastudio.google.com" target="_blank">https://datastudio.google.com</a> and create a new report. In the connectors section select &#34;BigQuery&#34; and navigate via your project and dataset to the tables &#34;dwh_product&#34;. Add this to the report.</p>
<p>Rename the report to &#34;BigData Course - Fundamentals - Sample DWH&#34;.</p>
<p>Select &#34;add data&#34; and add the second table (dwh_sales):</p>
<p class="image-container"><img style="width: 624.00px" src="img\\69135290084f315e.png"></p>
<h2 is-upgraded><strong>Generation of a Join in Data Studio</strong></h2>
<aside class="warning"><p><strong>Please note: </strong>Usually, BI-tools (like PowerBI, Qlik, Tableau, etc.) allow simple identification and modeling of Joins in some kind of &#34;data modeler&#34;. Data Studio also offers these capabilities, however in a somewhat unusual way of defining one new data source per join. </p>
</aside>
<p>Select &#34;Resource&#34; and manage linked data:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\12736aed8cb5356c.png"></p>
<p>Add this join to our data model (join attribute is the column &#34;product&#34;; add the dimensions product_name and date/sales_value from both tables). Call the joined table &#34;join_product_sales&#34; and save it:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\47910a730da7d630.png"></p>
<p>We can now use fields from both tables (dimension and fact) to, for example, create a line chart of sales over time:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\56842e78f10a77b5.png"></p>
<h2 is-upgraded><strong>Notes on costs for BigQuery</strong></h2>
<p>You will probably not notice any costs for BigQuery throughout this course, since you&#39;ll stay below the threshold of queries that are for free. Also, we are storing such low data volumes in our use cases currently, that the storage costs will be negligible.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="1">
        <p>Congratulations, you completed the third lab in our big data fundamentals course. This lab focused on setting up components for a more &#34;traditional&#34; BI architecture. This is, however, very important since the big data architecture will augment the BI architecture, but not replace it completely. Knowing how to set up &#34;normal&#34; ETL processes is very important for all analytics-related projects.</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab. You may keep all components since they do not induce huge costs or costs at all.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
