
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Batch Ingestion: Ingest RDBMS Source with Sqoop to Avro/Parquet&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="batch_ingestion_sqoop_avroparquet"
                  title="Lab &#34;Batch Ingestion: Ingest RDBMS Source with Sqoop to Avro/Parquet&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <aside class="special"><p><strong>Please note:</strong> The use case and DataProc setup steps are identical to the lab &#34;Batch Ingestion - Ingest RDBMS Source with Sqoop&#34;. If your cluster is still running you can skip the step &#34;Setting up the Hadoop DataProc Cluster&#34;.</p>
</aside>
<p>In this use case we want to store the contents of our webshop table in the data lake (nightly full extract).</p>
<p class="image-container"><img style="width: 624.00px" src="img\\35717875effa5606.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Ingesting structured relational data into the data lake for batch processing via a (horizontally) scalable sqoop (MapReduce) job using the popular big data formats Avro and Parquet.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Set up a DataProc cluster which is capable of executing the sqoop job and act as a data lake (HDFS).</li>
<li>Learn about initialization actions for Hadoop DataProc clusters and especially use the sqoop initialization action.</li>
<li>Start a sqoop job to transfer data from cloud SQL to the data lake (HDFS-/GCS-based) - this time however storing the files in Avro/Parquet instead of plain text.</li>
</ul>
<aside class="warning"><p><strong>Please note:</strong> You&#39;ll need GCP access for this lab. The provided voucher needs to be redeemed. Remember to shut down the running cloud function after completing!</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Setting up the Hadoop DataProc Cluster" duration="4">
        <p>Please set up the cluster according to the lab <a href="https://pkuep.github.io/pk-bigdata/batch_ingestion_sqoop" target="_blank">RDBMS to GCS (Avro/Parquet) with sqoop</a> </p>


      </google-codelab-step>
    
      <google-codelab-step label="Creating the Sqoop Ingestion Job with Parquet Output" duration="5">
        <h2 is-upgraded><strong>Executing the job</strong></h2>
<p>Let&#39;s now start the sqoop job with a further parameter &#34;--as-parquetfile&#34;:</p>
<pre><code>sqoop import --connect jdbc:mysql://&lt;db-IP see Moodle&gt;/&lt;dbname, see Moodle&gt; --userna
me &lt;see Moodle&gt; --password &lt;see Moodle&gt; --table sales --m 1 --target-dir gs://hdm-kueppers/sales_from_sqoop_parquet --as-parquetfile</code></pre>
<h2 is-upgraded><strong>Checking the results</strong></h2>
<p>The sqoop job should create the new folder within your bucket.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\88513113efeab703.png"></p>
<h2 is-upgraded><strong>Inspecting the Parquet File</strong></h2>
<p>Let&#39;s take a quick look at the Parquet file in a notebook:</p>
<pre><code>import pandas as pd
df = pd.read_parquet(&#34;gs://hdm-kueppers/sales_from_sqoop_tst_parquet/part-m-00000.parquet&#34;)
print(df.dtypes)
df.head(5)</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\9164306ebbace684.png"></p>
<p>Do you see any issues?</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You executed a sqoop (MapReduce) job with a further configuration (output as Parquetfile).</li>
<li>This setup is big data ready for heavy workloads, since MapReduce is capable of scaling out (horizontally) and if necessary, you can add worker nodes to your cluster (however, the SQL database might become a bottleneck).</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Creating the Sqoop Ingestion Job with Avro Output" duration="3">
        <h2 is-upgraded><strong>Executing the job</strong></h2>
<p>Let&#39;s now start the sqoop job with a further parameter &#34;--as-avrodatafile&#34;:</p>
<pre><code>sqoop import --connect jdbc:mysql://&lt;db-IP see Moodle&gt;/&lt;dbname, see Moodle&gt; --userna
me &lt;see Moodle&gt; --password &lt;see Moodle&gt; --table sales --m 1 --target-dir gs://hdm-kueppers/sales_from_sqoop_avro --as-avrodatafile</code></pre>
<h2 is-upgraded><strong>Checking the results</strong></h2>
<p>Open the notebook you used previously and install fastavro first:</p>
<pre><code>!pip install fastavro</code></pre>
<p>Next, we can read the avro file:</p>
<pre><code>import pandas as pd
from fastavro import reader
import gcsfs

# Define the GCS path to the Avro file
gcs_path = &#39;path_to_your_avro_file&#39;

# Create GCS file system object
fs = gcsfs.GCSFileSystem(project=&#39;your-gcp-project-id&#39;)

# Open and read the Avro file
with fs.open(gcs_path, &#39;rb&#39;) as f:
    avro_reader = reader(f)
    records = list(avro_reader)

# Convert to DataFrame
df = pd.DataFrame(records)

print(df.dtypes)
df.head()
</code></pre>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You executed a sqoop (MapReduce) job with a further configuration (output as Avrofile).</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Loading Parquet Files in PySpark" duration="8">
        <h2 is-upgraded><strong>&#34;Big data&#34; load in PySpark</strong></h2>
<h3 is-upgraded><strong>Opening a PySpark session</strong></h3>
<p>Although we have not learned much about Spark yet, we want to take a quick glance at how a Parquet file can be loaded into our cluster&#39;s worker nodes for distributed (big data) processing.</p>
<p>In the shell you used for executing the sqoop jobs (i.e. the cluster master node), please enter the following command to start an interactive PySpark session:</p>
<pre><code>pyspark</code></pre>
<p>After few seconds, you should see the interactive PySpark shell:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\9363fda12fb15c44.png"></p>
<h3 is-upgraded><strong>Directing the cluster towards the Parquet files</strong></h3>
<p>With the following command, we can tell the spark cluster that there is a Parquet file in our GCS (or also HDFS) and inspect the schema:</p>
<pre><code>df = spark.read.load(&#34;gs://hdm-kueppers/sales_from_sqoop_tst_parquet/*.parquet&#34;)
df</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\988414435d1fbd6b.png"></p>
<p>With the following command, we can inspect the first three rows:</p>
<pre><code>df.head(3)</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\aa2aaa126a86f9e3.png"></p>
<p>Please close the interactive PySpark shell with this command:</p>
<pre><code>exit()</code></pre>
<p>This should be the output:</p>
<aside class="special"><p><strong>Please note:</strong> The DataFrame is not loaded into the memory of one node (like in the Jupyter/Pandas case above) but distributed across all worker nodes of the cluster. We&#39;ll learn how Spark works later but should keep in mind that the framework we used here allows big data and distributed processing.</p>
</aside>
<aside class="warning"><p><strong>Please note:</strong> We did not work with the Avro files since (1) we are more interested in analytical processing and hence Parquet storage and (2) reading Avro is slightly more complex in both, Pandas and PySpark.</p>
</aside>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You loaded Parquet files from cloud storage into the memory of a JupyterLab (AI Platform) virtual machine using Pandas.</li>
<li>You loaded Parquet files from cloud storage into a Spark cluster using the interactive PySpark shell on the DataProc cluster&#39;s master node.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Cleaning Up" duration="2">
        <p>Please make sure to delete your cluster (in the live-lecture, please leave your cluster running):</p>
<p class="image-container"><img style="width: 624.00px" src="img\\be443522e927f2d8.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You finished the lab and performed all necessary clean-up tasks.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="2">
        <p>Congratulations, you set up a &#34;big data-traditional&#34; and horizontally scaling ingestion pipeline using a Hadoop cluster with sqoop and stored your data in popular big data formats (Avro and Parquet). You also got a first impression of how to access these file formats from Pandas and PySpark (which you&#39;ll get to know in more detail later).</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
