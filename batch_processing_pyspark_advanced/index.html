
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Batch Processing: PySpark SQL, DataFrames, and Machine Learning&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="batch_processing_pyspark_advanced"
                  title="Lab &#34;Batch Processing: PySpark SQL, DataFrames, and Machine Learning&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>In this simplified use case we want to start an interactive PySpark shell and perform the word count example.</p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Setup of a Dataproc cluster for further PySpark labs and execution of the map-reduce logic with spark..</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Set up a Dataproc cluster including a Jupyter notebook.</li>
<li>Opening</li>
<li>Reading the data lake and counting the words</li>
</ul>
<aside class="warning"><p><strong>Please note:</strong> You&#39;ll need GCP access for this lab. The provided voucher needs to be redeemed.</p>
</aside>
<aside class="warning"><p><strong>Requirements:</strong> The previous lab &#34;Pyspark Wordcount&#34; should have been executed such that you have a Dataproc Spark cluster available and are able to create PySpark Jupyter Notebooks.</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Creating a Hadoop Cluster with a Jupyter Notebook" duration="1">
        <p><strong>See previous Lab!</strong></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You configured and started your first big data system, a Hadoop cluster, called &#34;DataProc&#34; in GCP. DataProc is the basis for many big data ingestion, transformation, and processing tasks in GCP.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Accessing the Jupyter Notebook and Connecting to Spark" duration="1">
        <h2 is-upgraded><strong>See previous Lab</strong></h2>
<p>Please create a Jupyter Notebook (PySpark) on GCS and call it &#34;BigData - Batch Processing - PySpark SQL, DataFrames, and ML&#34;.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\cee2642280acf2fb.png"></p>
<h2 is-upgraded><strong>Connecting to the PySpark session and reading the data lake</strong></h2>
<p>Check if you are connected to the Spark master via the &#34;spark context&#34; variable sc:</p>
<pre><code>sc</code></pre>
<p>This should be the output: </p>
<p class="image-container"><img style="width: 624.00px" src="img\\19a9618f25082a01.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a Jupyter Notebook as an interactive big data tool for batch processing.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="PySpark Programming with SQL" duration="8">
        <h2 is-upgraded><strong>Connecting to a csv file in GCS (creating the DataFrame)</strong></h2>
<p>The variable &#34;sc&#34; allows you to interact with the Spark cluster in an &#34;RDD-manner&#34;. There is another variable &#34;spark&#34; that also allows interaction with the cluster (the so called SparkSession) &#34;spark&#34; in the &#34;DataFrame-manner&#34;:</p>
<p class="image-container"><img style="width: 98.43px" src="img\\a2db26bf1b6f0f0c.png"></p>
<p>This variable can be utilized to use the higher-level functions of Spark, especially Spark SQL and DataFrames. Let&#39;s create an entry point to our data lake for our Spark application:</p>
<pre><code># provide spark the link to the webshop history (datalake)
df = spark.read.csv(&#34;gs://pk-gcs/webshop_datalake/webshop_history.csv&#34;, inferSchema=True, header=True)  # network communication necessary</code></pre>
<p>We use the DataFrame environment here which you can check by executing a cell with just &#34;df&#34; in it:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\4d9cae634d9da59e.png"></p>
<h2 is-upgraded><strong>Registering a temporary table</strong></h2>
<p>In order to be able to use SQL, we need to register a temporary table (if you access a table in the Hive metastore, this is not necessary; see the respective labs on Hive, sqoop, and data ingestion)</p>
<pre><code># create a temporary view on the data that we can simply query by SQL
df.createOrReplaceTempView(&#34;sales&#34;)</code></pre>
<h2 is-upgraded><strong>Querying the temporary table</strong></h2>
<p>You can now execute (more or less) standard SQL analyses using the sales table.</p>
<pre><code># calculate the mean sales_value per product_name
# we use &#34;toPandas&#34; for a better visualization (instead of collect)
spark.sql(&#34;SELECT product_name, mean(sales_value) FROM sales GROUP BY product_name&#34;).toPandas()</code></pre>
<p>This should be the result:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\99422c5a753f99f5.png"></p>
<h2 is-upgraded><strong>Your Analytical Task</strong></h2>
<p>Please write and execute a SQL-statement that allows to compare the average sales value between the age_groups ‘18-29&#39; and ‘30-49&#39; for the products &#34;red wine&#34; and &#34;pils&#34; (order the output by product_name).</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You used a Jupyter Notebook as an interactive big data tool for batch processing to perform SQL-based queries.</li>
<li>The code you ran is big data-ready and scalable.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="PySpark Programming with DataFrames" duration="8">
        <h2 is-upgraded><strong>The DataFrame API</strong></h2>
<p>Please take a look at the DataFrame API documentation: <a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html" target="_blank">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html</a> </p>
<h2 is-upgraded><strong>Your Analytical Task</strong></h2>
<p>Your task is to execute the same logic as above using the DataFrame API: &#34;compare the average sales value between the age_groups ‘18-29&#39; and ‘30-49&#39; for the products &#34;red wine&#34; and &#34;pils&#34; (order the output by product_name&#34;.</p>
<p>Hint: you can filter rows for example with this code:</p>
<pre><code># logical or to filter a DataFrame
df.filter((df.age_group==&#39;18-29&#39;) | (df.age_group==&#39;30-49&#39;)).toPandas().head()</code></pre>
<p>For aggregation, the &#34;agg&#34;-method is recommended. Sorting can be achieved with &#34;sort&#34;.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You used the PySpark DataFrame API to implement a query.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Machine Learning with PySpark" duration="8">
        <h2 is-upgraded><strong>The Machine Learning API</strong></h2>
<p>Please take a look at the Spark ML API documentation: <a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html" target="_blank">https://spark.apache.org/docs/latest/api/python/pyspark.ml.html</a> </p>
<h2 is-upgraded><strong>Your Analytical Task</strong></h2>
<p>We want to train a machine learning model (Gradient Boosted Regression) with the features &#34;age&#34; and &#34;product&#34; and the target &#34;sales_value&#34;. We will skip advanced machine learning concepts here but take a look at the core challenges in training a model in Spark.</p>
<h3 is-upgraded><strong>Generating the Feature Vector</strong></h3>
<p>Spark requires a specific column &#34;features&#34; which is actually the feature matrix. With the following code, we transfer the two columns &#34;age&#34; and &#34;product&#34; to one &#34;vector assembled&#34; column &#34;features&#34; in a new DataFrame df_ml.</p>
<pre><code># generate the feature column
from pyspark.ml.feature import VectorAssembler
va = VectorAssembler(inputCols=[&#39;age&#39;,&#39;product&#39;],outputCol=&#39;features&#39;)
df_ml = va.transform(df).select(&#39;features&#39;,&#39;sales_value&#39;)
df_ml.toPandas().head()</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 421.36px" src="img\\9e45b23f2035a128.png"></p>
<h3 is-upgraded><strong>Train-test-split</strong></h3>
<p>The train-test-split can be done as follows:</p>
<pre><code># split the data into training and test sets (20% held out for testing)
(train, test) = df_ml.randomSplit([0.8, 0.2])
</code></pre>
<h3 is-upgraded><strong>Training the model</strong></h3>
<pre><code># train the model
from pyspark.ml.regression import GBTRegressor
reg = GBTRegressor(featuresCol=&#39;features&#39;, labelCol=&#39;sales_value&#39;)
model = reg.fit(train)</code></pre>
<h3 is-upgraded><strong>Making predictions</strong></h3>
<p>For model evaluation and demonstration purposes, we can make predictions on the test data:</p>
<pre><code>#make predictions using the test data
predictions = model.transform(test)

# select some example rows 
predictions.toPandas().sample(10)</code></pre>
<h3 is-upgraded><strong>Evaluating the model</strong></h3>
<p>We can use the test data-predictions for model evaluation as follows:</p>
<pre><code># evaluate the model
from pyspark.ml.evaluation import RegressionEvaluator

# select (prediction, true label) and compute test error
evaluator = RegressionEvaluator(labelCol=&#34;sales_value&#34;, predictionCol=&#34;prediction&#34;, metricName=&#34;rmse&#34;)
rmse = evaluator.evaluate(predictions)
print(&#34;Root Mean Squared Error (RMSE) on test data = %g&#34; % rmse)</code></pre>
<h2 is-upgraded><strong>Improving the model with a further column</strong></h2>
<p>The following code shows some examples of how to handle string columns and perform a one-hot-encoding before model training. We don&#39;t go into details on this.</p>
<pre><code># improve the model with a further (one-hot-encoded) feature
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.feature import StringIndexer

# convert string to numerical column
indexer = StringIndexer(inputCol=&#34;weekday&#34;, outputCol=&#34;weekday_idx&#34;)
df_indexed = indexer.fit(df).transform(df)

# encode weekday index (one-hot-encoding) and select only relevant columns
encoder = OneHotEncoder(inputCol=&#39;weekday_idx&#39;, outputCol=&#39;weekday_encoded&#39;)
encoded_df = encoder.transform(df_indexed).select(&#39;weekday_encoded&#39;,&#39;age&#39;,&#39;product&#39;,&#39;sales_value&#39;)

# create feature column
va = VectorAssembler(inputCols=[&#39;age&#39;, &#39;product&#39;, &#39;weekday_encoded&#39;],outputCol=&#39;features&#39;)
df_ml = va.transform(encoded_df).select(&#39;features&#39;,&#39;sales_value&#39;)

(train, test) = df_ml.randomSplit([0.8, 0.2])

reg = GBTRegressor(featuresCol=&#39;features&#39;, labelCol=&#39;sales_value&#39;)
model = reg.fit(train)

predictions = model.transform(test)

evaluator = RegressionEvaluator(labelCol=&#34;sales_value&#34;, predictionCol=&#34;prediction&#34;, metricName=&#34;rmse&#34;)
rmse = evaluator.evaluate(predictions)
print(&#34;Root Mean Squared Error (RMSE) on test data (with weekday column) = %g&#34; % rmse)</code></pre>
<p>The RMSE should be slightly better with the additional feature.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You used the PySpark with the ML-package to train a model, make predictions, and evaluate the model using our Dataproc cluster.</li>
<li>This setup is big data machine learning ready!</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Shutting Down the Cluster" duration="1">
        <h2 is-upgraded><strong>Shutting down the DataProc Cluster</strong></h2>
<p>Go to the cluster details page and hit &#34;Delete&#34; (in the live lecture, please leave it running):</p>
<p class="image-container"><img style="width: 541.50px" src="img\\4f27ebbf007e1184.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="1">
        <p>Congratulations, you learned how to work with PySpark in the modern Dataframe-like (including SQL) programming style and how to train and evaluate a machine learning model.</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
