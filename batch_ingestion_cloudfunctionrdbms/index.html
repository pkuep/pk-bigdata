
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Batch Ingestion: Ingest RDBMS Source with a Cloud Function&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="batch_ingestion_cloudfunctionrdbms"
                  title="Lab &#34;Batch Ingestion: Ingest RDBMS Source with a Cloud Function&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <p>In this use case we want to store the contents of our webshop table in the data lake (nightly full extract).</p>
<p class="image-container"><img style="width: 624.00px" src="img\\87c5f66764ab2204.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Ingesting structured relational data into the data lake for batch processing via a (vertically) scalable cloud function.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Take a look at the Python logic to connect to a SQL server.</li>
<li>Transfer the Python logic from a local machine to the cloud function.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Connecting to SQL in Python (example in a notebook)" duration="5">
        <h2 is-upgraded><strong>Creating a cloud-based Jupyter Notebook environment</strong></h2>
<p>If you don&#39;t have Anaconda and Jupyter Notebook installed        locally on your computer, please create a notebook in GCP&#39;s AI Platform using Vertex AI / Colab Enterprise:</p>
<p class="image-container"><img style="width: 239.00px" src="img\\487dbf8b21fdf691.png"></p>
<h2 is-upgraded><strong>Creating the Notebook and Implementing the Logic</strong></h2>
<p>Create a cell and insert the following code to install the mysql driver:</p>
<pre><code>!pip install pymysql</code></pre>
<p>Import the newly installed package pymysql in another cell:</p>
<pre><code>import pymysql</code></pre>
<p>Hit &#34;Copy&#34; and add the following new cell to your JupyterLab (replace &lt;see Moodle&gt; with the respective information provided in the section &#34;Organization&#34;):</p>
<pre><code>conn = pymysql.connect(host=&#39;&lt;see Moodle&gt;&#39;, user=&lt;see Moodle&gt;, password=&#39;&lt;see Moodle&gt;&#39;, db=&#39;&lt;see Moodle&gt;&#39;)
conn</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\b3c7800dc87a39f8.png"></p>
<h2 is-upgraded><strong>Retrieve all rows from the table &#34;sales&#34;</strong></h2>
<p>Next, we want to get all rows from the table for visualization purposes:</p>
<pre><code>with conn.cursor() as cur:
    cur.execute(&#39;SELECT * FROM sales&#39;)
    rows = cur.fetchall()
    sales = &#34;date, weekday, region, age_group, product, product_group, sales_value\n&#34;
    for row in rows:
        sales += (f&#39;{row[0]}, {row[1]}, {row[2]}, {row[3]}, {row[4]}, {row[5]}, {row[6]}\n&#39;)
        
print(sales)</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\ae3452538b71565.png"></p>
<h2 is-upgraded><strong>Store table &#34;sales&#34; in cloud storage</strong></h2>
<p>In case you are working in the cloud JupyterLab, you can access the cloud storage easily with the following code:</p>
<pre><code>from datetime import datetime
from google.cloud import storage
filename = f&#34;webshop_sales_{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}.csv&#34;
storage_client = storage.Client()
bucket = storage_client.get_bucket(&#34;hdm-kueppers&#34;)
blob = bucket.blob(filename)

blob.upload_from_string(sales, content_type=&#39;text/csv&#39;)</code></pre>
<p>You should now be able to see the csv file in the cloud console (under Cloud Storage)</p>
<p class="image-container"><img style="width: 569.00px" src="img\\983f939b7aeae6e5.png"></p>
<p>Downloading and opening the file should show all contents of the table &#34;sales&#34;:</p>
<p class="image-container"><img style="width: 389.13px" src="img\\3060d898207085a9.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a Jupyter Notebook in order to understand the requirements of our ingestion logic: pulling all data from a table via the pymysql connector and pushing these to the data lake.</li>
<li>You set up a Notebook environment in the cloud which is capable of accessing different resources in your GCP project, especially the cloud SQL server and cloud storage.</li>
<li>You pulled all data from a sample table and (1) showed it in the notebook and (2) stored it on GCS, i.e. the data lake.</li>
<li>Next, we want to transfer the logic to a cloud function.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Creating and Developing the Cloud Function" duration="9">
        

      </google-codelab-step>
    
      <google-codelab-step label="Creating and Developing the Cloud Function" duration="9">
        <h2 is-upgraded><strong>Creating a cloud function for image retrieval and storage upload</strong></h2>
<p>Please go to cloud functions in the console:</p>
<p class="image-container"><img style="width: 270.00px" src="img\\1b888a627ffe6857.png"></p>
<p>Create a function:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\d21990bcaa67de4a.png"></p>
<p>We&#39;ll use the inline editor:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\39d7ecef3f72796d.png"></p>
<p>Set the function (=service) name to &#34;rdbms-ingestion-yourintials&#34;. Choose us-central1 as region and a modern Python runtime. Set &#34;Allow public access&#34; under &#34;Authentication&#34;. The other parameters can be set as default. Hit create.</p>
<p>Next, let&#39;s transfer the logic of the Jupyter Notebook to our method &#34;rdbms-ingestion-yourintials&#34;.</p>
<pre><code>from google.cloud import storage
from datetime import datetime
import pymysql
 
def ingest_webshop(request):
    &#34;&#34;&#34; connects to the webshop database, retrieves all rows from the
        table &#34;sales&#34; and uploads these into a file on GCS
    &#34;&#34;&#34;
    # connect to the database - replace &lt;cloud SQL IP&gt; with the IP of your cloud SQL server
    conn = pymysql.connect(host=&#39;&lt;see Moodle&gt;&#39;, user=&lt;see Moodle&gt;, password=&#39;&lt;see Moodle&gt;&#39;, db=&#39;&lt;see Moodle&gt;&#39;)

 
    # create a string that holds all rows of the table &#34;sales&#34; in a csv-ready format
    with conn.cursor() as cur:
        cur.execute(&#39;SELECT * FROM sales&#39;)
        rows = cur.fetchall()
        sales = &#34;date, weekday, region, age_group, product, product_group, sales_value\n&#34;
        for row in rows:
            sales += (f&#39;{row[0]}, {row[1]}, {row[2]}, {row[3]}, {row[4]}, {row[5]}, {row[6]}\n&#39;)
 
    # create a filename with a timestamp and store the data in a csv file in the datalake
    filename = f&#34;webshop_sales_fromfunction_{datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)}.csv&#34;
    storage_client = storage.Client()
    bucket = storage_client.get_bucket(&#34;pk-gcs&#34;)
    blob = bucket.blob(filename)
 
    blob.upload_from_string(sales, content_type=&#39;text/csv&#39;)

    return &#39;Success&#39;


</code></pre>
<aside class="warning"><p><strong>Warning:</strong> Storing credentials like this is not the recommended way of connecting to a database or invoking other services. Usually one sets up a service account for this or the sensitive data is stored in a specific repository (e.g. GCP Secret Manager, see <a href="https://cloud.google.com/secret-manager" target="_blank">https://cloud.google.com/secret-manager</a>). </p>
</aside>
<p>Next, we&#39;ll need to specify that our cloud function requires some Python packages. Click on &#34;requirements.txt&#34; and add this code at the end of the file:</p>
<pre><code>google-cloud-storage
datetime
pymysql</code></pre>
<p>Hit &#34;Deploy&#34; (either from requirements.txt or main.py):</p>
<p class="image-container"><img style="width: 624.00px" src="img\\923da52c81698c84.png"></p>
<p>Wait 1-2 minutes until the final function is deployed and then click on the URL.</p>
<p>You should now see the output in your bucket:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\ede123d0329e0b97.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You transferred the sample logic from the notebook to a cloud function (main.py).</li>
<li>You specified requirements on packages of your function (requirements.txt).</li>
<li>You deployed the function and tested it.</li>
<li>The ingestion logic (i.e. the cloud function) is now ready to be scheduled regularly.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Scheduling the Cloud Function" duration="1">
        <p>We set up a cloud scheduler in the lab &#34;Image to GCS with Cloud Functions&#34; (see <a href="https://pkuep.github.io/pk-bigdata/batch_ingestion_cloudfunctionimage" target="_blank">https://pkuep.github.io/pk-bigdata/batch_ingestion_cloudfunctionimage</a>). The steps to automate this function are the same. Thus, we&#39;ll spare them here.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Cleaning Up" duration="2">
        <p>Please make sure to delete your scheduled job in the cloud scheduler (if applicable):</p>
<p class="image-container"><img style="width: 624.00px" src="img\\b8a5636659fd5994.png"></p>
<p>Next, you may want to delete the files in the bucket.</p>
<p class="image-container"><img style="width: 624.00px" src="img\\f93f268fa80d7f59.png"></p>
<p>Next, you should delete the cloud function.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You finished the lab and performed all necessary clean-up tasks.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="2">
        <p>Congratulations, you set up a state-of-the-art cloud ingestion pipeline using cloud functions and scheduled its execution.</p>
<aside class="warning"><p><strong>Warning: </strong>Cloud functions are not &#34;auto-scaling&#34; and also not horizontally scaling as other technologies we have explored already and will explore in further detail (e.g. Spark, Apache Beam, Dask). Thus, if you want to scale out (e.g. to a table with 1 billion of rows), you might first need to scale vertically (provide the function more RAM) and then horizontally in a manual manner (e.g. by generating 100 cloud functions, each being responsible for the ingestion of 10 million rows). However, we&#39;ll learn about scaling batch ingestion methods later on.</p>
</aside>
<aside class="special"><p><strong>Hint: </strong>in order to avoid high costs for the notebook instance you should first develop in a local environment (e.g. using Anaconda) and then deploy to a cloud notebook or cloud function. However, accessing resources from outside the project (e.g. a notebook in GCP) is more complex. We will get to know how this works later (service accounts are necessary).</p>
</aside>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
