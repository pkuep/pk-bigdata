
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Lab &#34;Batch Ingestion: Ingest RDBMS Source with Sqoop and Hive&#34;</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid="None"
                  id="batch_ingestion_sqoop_hive"
                  title="Lab &#34;Batch Ingestion: Ingest RDBMS Source with Sqoop and Hive&#34;"
                  environment="web"
                  feedback-link="https://p-kueppers.com">
    
      <google-codelab-step label="Introduction" duration="0">
        <aside class="special"><p><strong>Please note:</strong> The use case and DataProc setup steps are identical to the lab &#34;Batch Ingestion - Ingest RDBMS Source with Sqoop&#34;. If your cluster is still running you can skip the step &#34;Setting up the Hadoop DataProc Cluster&#34;.</p>
</aside>
<p>In this use case we want to store the contents of our webshop table in the data lake (nightly full extract).</p>
<p class="image-container"><img style="width: 624.00px" src="img\\8339d6a3e3232d1a.png"></p>
<h3 is-upgraded><strong>Goal</strong></h3>
<p>Ingesting structured relational data into the data lake for batch processing via a (horizontally) scalable sqoop (MapReduce) job using the popular big data formats Avro and Parquet.</p>
<h2 is-upgraded><strong>What you&#39;ll implement</strong></h2>
<ul>
<li>Set up a DataProc cluster which is capable of executing the sqoop job and act as a data lake (HDFS).</li>
<li>Learn about initialization actions for Hadoop DataProc clusters and especially use the sqoop initialization action.</li>
<li>Start a sqoop job to transfer data from SQL to the data lake (HDFS/GCS-based) - this time however storing the files in Parquet instead of plain text.</li>
<li>Furthermore, you&#39;ll add information about the ingested data into the Hive meta warehouse.</li>
</ul>
<aside class="warning"><p><strong>Please note:</strong> You&#39;ll need GCP access for this lab. The provided voucher needs to be redeemed. Remember to shut down the running cloud function after completing!</p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Setting up the Hadoop DataProc Cluster" duration="4">
        <p>Please set up the cluster according to the lab <a href="https://pkuep.github.io/pk-bigdata/batch_ingestion_sqoop" target="_blank">RDBMS to GCS (Avro/Parquet) with sqoop</a> </p>
<p>Cluster creation will take 2-3 minutes.</p>
<h3 is-upgraded><strong>Connecting to the Hadoop master node</strong></h3>
<p>When cluster creation is finished, you can select the cluster in the web UI:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\a80f5f7db4d71775.png"></p>
<p>Open a connection to the master node which is able to start sqoop jobs:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\5f8b49055e3e4dfb.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You created a Hadoop cluster (DataProc) which provides functionality for accessing cloud SQL (initialization action &#34;cloud-sql-proxy&#34;) as well as the sqoop component for batch ingestion of relational data into our data lake.</li>
<li>Next, we&#39;ll use the connection to the master node and execute our sqoop data ingestion job.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Creating the Sqoop Ingestion Job with Parquet Output and Hive Import" duration="5">
        <h2 is-upgraded><strong>Preparation</strong></h2>
<p>First, we&#39;ll delete the output from our first sqoop run, since sqoop tries to write into the HDFS directory &#34;sales&#34; and will skip the job in case this already exists:</p>
<pre><code>hadoop fs -rm sales/*
hadoop fs -rmdir sales</code></pre>
<h2 is-upgraded><strong>Executing the job</strong></h2>
<p>Let&#39;s now start the sqoop job with a further parameter &#34;--hive-import&#34;:</p>
<aside class="warning"><p><strong>Important:</strong> This command will unfortunately <strong>not </strong>work due to DataProc issues in providing the correct libraries. We will therefore use a workaround below and NOT execute the sqoop job.</p>
</aside>
<pre><code>sqoop import --connect jdbc:mysql://&lt;db-IP see Moodle&gt;/&lt;dbname, see Moodle&gt; --userna
me &lt;see Moodle&gt; --password &lt;see Moodle&gt; --table sales --m 1 --hive-import</code></pre>
<h2 is-upgraded><strong>WORKAROUND: Manual Interaction with Hive and Data Ingestion</strong></h2>
<h3 is-upgraded><strong>Connecting to beeline</strong></h3>
<p>Due to the mentioned issue we will now start interacting with Hive directly in order to ingest data via &#34;standard SQL&#34; (like in the lab &#34;Simple Analytics Pipeline&#34;). As stated in the lecture, there exists a command line interface (CLI) for Hive called &#34;beeline&#34;.</p>
<aside class="special"><p><strong>Please note:</strong> There are more sophisticated Hive interfaces and Hive can also be accessed by almost all modern BI tools like PowerBI, Tableau etc.</p>
</aside>
<p>Please open the SSH connection to the master node and execute the following command in order to start Hive&#39;s CLI beeline and connect to our cluster&#39;s Hive metastore (running on the master node &#34;localhost&#34;):</p>
<pre><code>beeline -u &#34;jdbc:hive2://localhost:10000&#34;</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\4d96bbe5dee7b4b8.png"></p>
<h3 is-upgraded><strong>Creating a Parquet-based Hive table</strong></h3>
<p>Using Hive, we can interact with the (structured data in the) data lake via standard SQL. Hive will create respective files in the data lake (just like the sqoop command above would have). Executing the following command will create a folder in our data lake on cloud storage (under &#34;hive-warehouse&#34; called sales and will add a table to the Hive metastore. All data we will ingest will be stored as Parquet files.</p>
<pre><code>create table sales (
sales_date date,
weekday varchar(20),
region varchar(20),
age_group varchar(20),
product_name varchar(30),
product_group int,
sales_value float
) STORED AS PARQUET;</code></pre>
<p>The output should be &#34;No rows affected&#34;.</p>
<h3 is-upgraded><strong>Ingesting data (manually) into the Hive warehouse</strong></h3>
<p>After having created the table, we can ingest data - in this workaround manually. Let&#39;s use the same data as in the first lab (i.e. the same as in the cloud SQL database):</p>
<pre><code>insert into sales values
(&#39;2020-04-03&#39;, &#39;Friday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;whiskey&#39;, &#39;3&#39;, 21.5),
(&#39;2020-04-01&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 4.4),
(&#39;2020-04-01&#39;, &#39;Wednesday&#39;, &#39;South&#39;, &#39;18-29&#39;, &#39;red wine&#39;, &#39;1&#39;, 12.8),
(&#39;2020-05-28&#39;, &#39;Thursday&#39;, &#39;Middle&#39;, &#39;30-49&#39;, &#39;lager&#39;, &#39;2&#39;, 47.1),
(&#39;2020-06-10&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;grappa&#39;, &#39;3&#39;, 14.4),
(&#39;2020-06-11&#39;, &#39;Thursday&#39;, &#39;Middle&#39;, &#39;18-29&#39;, &#39;red wine&#39;, &#39;1&#39;, 13.5),
(&#39;2020-03-11&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;18-29&#39;, &#39;red wine&#39;, &#39;1&#39;, 4.0),
(&#39;2020-05-07&#39;, &#39;Thursday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;grappa&#39;, &#39;3&#39;, 17.3),
(&#39;2020-04-10&#39;, &#39;Friday&#39;, &#39;Middle&#39;, &#39;30-49&#39;, &#39;red wine&#39;, &#39;1&#39;, 10.6),
(&#39;2020-01-24&#39;, &#39;Friday&#39;, &#39;North&#39;, &#39;30-49&#39;, &#39;pils&#39;, &#39;2&#39;, 29.4),
(&#39;2020-04-13&#39;, &#39;Monday&#39;, &#39;North&#39;, &#39;18-29&#39;, &#39;pils&#39;, &#39;2&#39;, 54.0),
(&#39;2020-02-26&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;red wine&#39;, &#39;1&#39;, 4.5),
(&#39;2020-04-11&#39;, &#39;Saturday&#39;, &#39;Middle&#39;, &#39;18-29&#39;, &#39;whiskey&#39;, &#39;3&#39;, 45.3),
(&#39;2020-03-15&#39;, &#39;Sunday&#39;, &#39;South&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 16.7),
(&#39;2020-05-15&#39;, &#39;Friday&#39;, &#39;South&#39;, &#39;30-49&#39;, &#39;white wine&#39;, &#39;1&#39;, 32.7),
(&#39;2020-03-04&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;pils&#39;, &#39;2&#39;, 33.2),
(&#39;2020-05-10&#39;, &#39;Sunday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;rosé wine&#39;, &#39;1&#39;, 5.6),
(&#39;2020-02-19&#39;, &#39;Wednesday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 5.4),
(&#39;2020-03-04&#39;, &#39;Wednesday&#39;, &#39;Middle&#39;, &#39;30-49&#39;, &#39;rosé wine&#39;, &#39;1&#39;, 24.2),
(&#39;2020-05-28&#39;, &#39;Thursday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 13.3),
(&#39;2020-01-24&#39;, &#39;Friday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 14.9),
(&#39;2020-02-19&#39;, &#39;Wednesday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;whiskey&#39;, &#39;3&#39;, 34.9),
(&#39;2020-05-27&#39;, &#39;Wednesday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;whiskey&#39;, &#39;3&#39;, 45.6),
(&#39;2020-01-23&#39;, &#39;Thursday&#39;, &#39;South&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 73.0),
(&#39;2020-03-06&#39;, &#39;Friday&#39;, &#39;South&#39;, &#39;30-49&#39;, &#39;rosé wine&#39;, &#39;1&#39;, 5.1),
(&#39;2020-04-16&#39;, &#39;Thursday&#39;, &#39;North&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 4.4),
(&#39;2020-01-17&#39;, &#39;Friday&#39;, &#39;South&#39;, &#39;50-&#39;, &#39;white wine&#39;, &#39;1&#39;, 23.2),
(&#39;2020-01-12&#39;, &#39;Sunday&#39;, &#39;Middle&#39;, &#39;18-29&#39;, &#39;red wine&#39;, &#39;1&#39;, 4.0),
(&#39;2020-01-27&#39;, &#39;Monday&#39;, &#39;Middle&#39;, &#39;50-&#39;, &#39;red wine&#39;, &#39;1&#39;, 30.2),
(&#39;2020-01-01&#39;, &#39;Wednesday&#39;, &#39;Middle&#39;, &#39;30-49&#39;, &#39;red wine&#39;, &#39;1&#39;, 33.2);</code></pre>
<p>Ingestion will take approximately 30s (remember, Hive is based on MapReduce). </p>
<h2 is-upgraded><strong>Checking the results</strong></h2>
<p>When selecting all data from the sales table, you should see the following output in beeline:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\1330891e77f76dab.png"></p>
<p>Please hit Ctrl-D to exit beeline. Within HDFS, we now should see a respective folder:</p>
<pre><code>hadoop fs -ls /user/hive/warehouse</code></pre>
<p class="image-container"><img style="width: 624.00px" src="img\\2b2bee6210dffb21.png"></p>
<p>There, you&#39;ll be able to take a look at the data file which is augmented by Hive schema information:</p>
<pre><code>hadoop fs -cat /user/hive/warehouse/sales/000000_0</code></pre>
<p class="image-container"><img style="width: 624.00px" src="img\\f70235fdac4935e6.png"></p>
<p>→ Hive acts as a useful SQL-based interface to our data lake and allows the ingestion of big data (with sqoop in a scaling manner) including metadata in the Hive metastore.</p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>Instead of executing a sqoop (MapReduce) job with a further configuration (ingestion into the hive warehouse), you learned how to use the Hive CLI beeline to create a table and ingest data using &#34;standard&#34; SQL.</li>
<li>The sqoop setup would be big data ready for heavy workloads and Hive provides a good meta-layer on top of our data lake.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Reading the Hive Metastore in PySpark" duration="3">
        <p>PySpark is directly integrated with Hive. Thus, you can query respective tables in the data lake from Spark with the following command:</p>
<pre><code>df = spark.sql(&#34;select * from sales&#34;)
df.head()</code></pre>
<p>This should be the output:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\c96a489c2d982f49.png"></p>
<p>We will learn how to use PySpark in detail later. Please close the PySpark shell using exit().</p>
<aside class="warning"><p><strong>Please note:</strong> Since we use our cloud SQL database as Hive metastore and cloud storage as the warehouse directory (instead of HDFS), the data and metadata are persistent and setting up a DataProc cluster exactly as we did (especially with the Hive parameters) will allow accessing the data lake from newly set up clusters. </p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Cleaning Up" duration="2">
        <p>Please make sure to delete your cluster:</p>
<p class="image-container"><img style="width: 624.00px" src="img\\be443522e927f2d8.png"></p>
<h2 is-upgraded><strong>Results</strong></h2>
<ol type="1" start="1">
<li>You finished the lab and performed all necessary clean-up tasks.</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Congratulations" duration="2">
        <p>Congratulations, you set up a &#34;big data-traditional&#34; and horizontally scaling ingestion pipeline using a Hadoop cluster with sqoop and stored your data in a popular big data format (Parquet) and used Hive to manage your metadata of the data lake. You also got a first impression of how to access the data lake via Hive from PySpark (which you&#39;ll get to know in more detail later).</p>
<aside class="special"><p><strong>Please note: </strong>You can now close this lab.</p>
</aside>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
  <script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
  <script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
